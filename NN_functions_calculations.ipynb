{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f50066a-25cd-4ac8-be38-2b85800dd665",
      "metadata": {
        "id": "3f50066a-25cd-4ac8-be38-2b85800dd665"
      },
      "source": [
        "# <font color = 'blue'>**Manual function calculations for loss and gradients in neural networks:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IsWkg1CufBwg",
      "metadata": {
        "id": "IsWkg1CufBwg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74c5185-44f2-4fd7-9b4a-0a07c03b4bd8",
      "metadata": {
        "id": "b74c5185-44f2-4fd7-9b4a-0a07c03b4bd8"
      },
      "source": [
        "##  <font color = 'blue'>**Manual Gradient function calculation:**\n",
        "## $f(x,y) = \\frac{x + \\exp(y)}{\\log(x) + (x-y)^3}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128ca67b-ed40-41df-a44f-2559578fbbb4",
      "metadata": {
        "id": "128ca67b-ed40-41df-a44f-2559578fbbb4"
      },
      "outputs": [],
      "source": [
        "def fxy(x,y):\n",
        "    # calculating the numerator of the gradient\n",
        "    num = x + torch.exp(y)\n",
        "\n",
        "    #calculating the denominator\n",
        "    den = torch.log(x) + (x - y)**3\n",
        "\n",
        "    # Performing element-wise division of the numerator by the denominator\n",
        "    return num/den"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ADHy2FqfP8S",
      "metadata": {
        "id": "0ADHy2FqfP8S"
      },
      "outputs": [],
      "source": [
        "#Creating an example tensor set to work with the gradient function\n",
        "x = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "y = torch.tensor(4.0, requires_grad = True) # Ensuring requires_grad = True sets us up to calculate the gradient and store it for usage during backpropogation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ac84f3-ad49-470a-b380-be3bd6be301f",
      "metadata": {
        "id": "b4ac84f3-ad49-470a-b380-be3bd6be301f"
      },
      "outputs": [],
      "source": [
        "# Create a single-element tensor 'x' containing the value 3.0\n",
        "# make sure to set 'requires_grad=True' as you want to compute gradients with respect to this tensor during backpropagation\n",
        "x = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "# Create a single-element tensor 'y' containing the value 4.0\n",
        "# Similar to 'x', we want to compute gradients for 'y' during backpropagation, hence make sure to set 'requires_grad=True'\n",
        "y = torch.tensor(4.0, requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_2HFXif6fg7A",
      "metadata": {
        "id": "_2HFXif6fg7A",
        "outputId": "5a1aa519-b743-4872-f331-cc8914bcaf6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(584.0868, grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the function 'fxy' with the tensors 'x' and 'y' as arguments\n",
        "# The result 'f' will also be a tensor and will contain derivative information because 'x' and 'y' have 'requires_grad=True'\n",
        "f = fxy(x, y)\n",
        "f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TbZiDeTXfrLf",
      "metadata": {
        "id": "TbZiDeTXfrLf"
      },
      "outputs": [],
      "source": [
        "# Perform backpropagation to compute the gradients of 'f' with respect to 'x' and 'y'\n",
        "# Hint use backward() function on f\n",
        "f.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7al8BVZKf3R7",
      "metadata": {
        "id": "7al8BVZKf3R7",
        "outputId": "f237ae61-fd98-4cdd-b971-917078b1d841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad = tensor(-19733.3965)\n",
            "y.grad = tensor(18322.8477)\n"
          ]
        }
      ],
      "source": [
        "# Display the computed gradients of 'f' with respect to 'x' and 'y'\n",
        "# These gradients are stored as attributes of x and y after the backward operation\n",
        "# Print the gradients for x and y\n",
        "print('x.grad =', x.grad)\n",
        "print('y.grad =', y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13326435-3e84-4885-b03e-2454e72208fd",
      "metadata": {
        "id": "13326435-3e84-4885-b03e-2454e72208fd"
      },
      "source": [
        "## <font color = 'blue'>**Implementing the -log of the softmax function for classification probabilities:**\n",
        "\n",
        "We're going to break down the calculation of classification probabilities using the softmax function, followed by the negative log to get that all-important loss function for a model:\n",
        "\n",
        "$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$\n",
        "\n",
        "So, what's happening here?\n",
        "\n",
        "- First, the function calculates the softmax probability, essentially turning our logits into probabilities.\n",
        "- Next, we apply the natural logarithm to this probability.\n",
        "- Finally, multiplying by `-1` gives us the negative log-likelihood, which is our **cross-entropy loss**.\n",
        "\n",
        "This is super important for multi-class classification problems because it helps us measure how well (or how badly) the model is predicting the true class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f95de5-f773-4872-93cc-58dbb27cedb9",
      "metadata": {
        "id": "f1f95de5-f773-4872-93cc-58dbb27cedb9"
      },
      "outputs": [],
      "source": [
        "# Loss function definition\n",
        "def log_exp(x,y):\n",
        "    # defining the numerator\n",
        "    num = torch.exp(x)\n",
        "\n",
        "    # defining the denominator\n",
        "    den = torch.exp(x)+torch.exp(y)\n",
        "\n",
        "    # passing the positive log of the num/den\n",
        "    pos_log = torch.log(num/den)\n",
        "\n",
        "    # don't forget to multiply by -1 to calculate loss\n",
        "    return pos_log * (-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada0LKKYC3c3",
      "metadata": {
        "id": "ada0LKKYC3c3"
      },
      "source": [
        "Test with normal inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oIZ0UETBC3c3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.215579Z",
          "start_time": "2019-01-29T22:48:56.209659Z"
        },
        "id": "oIZ0UETBC3c3",
        "outputId": "6b407f01-e34f-4532-9e74-137950497424"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.3133])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create tensors x and y with initial values 2.0 and 3.0, respectively\n",
        "x, y = torch.tensor([2.0]), torch.tensor([3.0])\n",
        "\n",
        "# Evaluate the function log_exp() for the given x and y, and store the output in z\n",
        "z = log_exp(x, y)\n",
        "\n",
        "# Display the computed value of z\n",
        "z\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ac1ffd-c230-473d-8738-d24a65ad17a5",
      "metadata": {
        "scrolled": true,
        "id": "e2ac1ffd-c230-473d-8738-d24a65ad17a5"
      },
      "source": [
        "## <font color='blue'>**Function Explanation: Computing Gradients Using PyTorch Autograd**</font>\n",
        "\n",
        "The function `grad` defined below takes two given tensors, `x` and `y`, and enables <font color='blue'>**gradient tracking**</font> for them by setting their `requires_grad` attribute to `True`. This is essential for PyTorch's automatic differentiation engine, <font color='blue'>**Autograd**</font>, to track operations on these tensors and compute gradients with respect to them.\n",
        "\n",
        "The function then passes `x` and `y` through a user-defined `forward_func`, which computes a scalar output `z`. This forward function represents any differentiable operation or model that the user wishes to analyze.\n",
        "\n",
        "After obtaining the output `z`, the function performs the <font color='blue'>**backward pass**</font> by calling `z.backward()`. This computes the <font color='blue'>**gradients**</font> of `z` with respect to all tensors that have `requires_grad=True` (in this case, `x` and `y`). The computed gradients are stored in the `.grad` attribute of each tensor.\n",
        "\n",
        "The function then prints out the gradients `x.grad` and `y.grad` to display the results of the computation.\n",
        "\n",
        "To prevent <font color='blue'>**gradient accumulation**</font>—which is the default behavior in PyTorch where gradients are added to any existing gradients in the `.grad` attributes—the function resets the gradients of `x` and `y` back to zero using the `zero_()` method. This ensures that subsequent calls to the `grad` function start with fresh gradients, avoiding any unintended interference from previous computations.\n",
        "\n",
        "**Key Points Demonstrated in the Function:**\n",
        "\n",
        "- **Enabling Gradient Tracking:** Using `x.requires_grad_(True)` and `y.requires_grad_(True)` to allow <font color='blue'>**Autograd**</font> to monitor operations on `x` and `y`.\n",
        "\n",
        "- **Forward Pass:** Passing the inputs through a customizable forward function `forward_func` to compute the output `z`.\n",
        "\n",
        "- **Backward Pass:** Calling `z.backward()` to compute the <font color='blue'>**gradients**</font> of `z` with respect to `x` and `y`.\n",
        "\n",
        "- **Accessing Gradients:** Printing `x.grad` and `y.grad` to access the computed gradients.\n",
        "\n",
        "- **Preventing Gradient Accumulation:** Resetting the gradients using `x.grad.zero_()` and `y.grad.zero_()` to avoid accumulation in successive computations.\n",
        "\n",
        "This implementation showcases an understanding of PyTorch's automatic differentiation mechanism and best practices in managing <font color='blue'>**gradient computations**</font>, ensuring accurate and efficient gradient calculations for optimization and analysis tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2kMC5IdJC3c4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.223303Z",
          "start_time": "2019-01-29T22:48:56.218056Z"
        },
        "id": "2kMC5IdJC3c4"
      },
      "outputs": [],
      "source": [
        "def grad(forward_func, x, y):\n",
        "\n",
        "  # Enable gradient tracking for x and y, set reauires_grad appropraitely\n",
        "  x.requires_grad_(True)\n",
        "  y.requires_grad_(True)\n",
        "\n",
        "  # Evaluate the forward function to get the output 'z'\n",
        "  z = forward_func(x, y)\n",
        "\n",
        "  # Perform the backward pass to compute gradients\n",
        "  z.backward()\n",
        "\n",
        "  # Print the gradients for x and y\n",
        "  print('x.grad =', x.grad)\n",
        "  print('y.grad =', y.grad)\n",
        "\n",
        "  # Reset the gradients for x and y to zero for the next iteration\n",
        "  x.grad.zero_()\n",
        "  y.grad.zero_()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g5Q3d5opC3c4",
      "metadata": {
        "id": "g5Q3d5opC3c4"
      },
      "source": [
        "Testing the function and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oob8ND3BC3c4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.267165Z",
          "start_time": "2019-01-29T22:48:56.227035Z"
        },
        "id": "oob8ND3BC3c4",
        "outputId": "dd81f1d4-cea9-49bb-ac82-255063572d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad = tensor([-0.7311])\n",
            "y.grad = tensor([0.7311])\n"
          ]
        }
      ],
      "source": [
        "grad(log_exp, x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L4nMM2joC3c4",
      "metadata": {
        "id": "L4nMM2joC3c4"
      },
      "source": [
        "But now let's try some \"hard\" inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d_f5Xk41C3c4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.285842Z",
          "start_time": "2019-01-29T22:48:56.274079Z"
        },
        "id": "d_f5Xk41C3c4"
      },
      "outputs": [],
      "source": [
        "x, y = torch.tensor([50.0]), torch.tensor([100.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21444428-08c7-4232-8e5c-fd8920fb72c4",
      "metadata": {
        "id": "21444428-08c7-4232-8e5c-fd8920fb72c4"
      },
      "outputs": [],
      "source": [
        "x,y = torch.tensor([50.0]), torch.tensor([100.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ECgqQ_i-C3c4",
      "metadata": {
        "id": "ECgqQ_i-C3c4",
        "outputId": "df08f86b-3243-4e65-bccf-e81385412781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad = tensor([nan])\n",
            "y.grad = tensor([nan])\n"
          ]
        }
      ],
      "source": [
        "grad(log_exp, x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kHsyDVblC3c4",
      "metadata": {
        "id": "kHsyDVblC3c4",
        "outputId": "4a92e217-cf37-4f73-db04-b229d147ba8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([inf])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.exp(torch.tensor([100.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0f6f15-127a-442e-ba72-888c3367475e",
      "metadata": {
        "id": "bc0f6f15-127a-442e-ba72-888c3367475e"
      },
      "source": [
        "### <font color = 'blue'>Explanation of Numerical Overflow and the Log-Sum-Exp Trick\n",
        "\n",
        "The reason for these output values is that we are experiencing a **numerical overflow** when we calculate $ e^{100}$, which is equal to $2.688 \\times 10^{43}$.  \n",
        "This value is not within range of our current datatype of `float32`. To fix this issue, we need to more stably calculate the values of the large exponential pieces.\n",
        "\n",
        "The way we can do this is through the **logarithmic identity principles**.\n",
        "\n",
        "- We know that $\\log\\left(\\frac{\\exp(x)}{\\exp(x) + \\exp(y)}\\right) = \\log(\\exp(x)) - \\log(\\exp(x) + \\exp(y))$\n",
        "- Also, $\\log(\\exp(x)) = x$\n",
        "- So our new equation would be $x - \\log(\\exp(x) + \\exp(y))$\n",
        "- Now, we can use a log-sum trick, understanding that $\\log(\\exp(x) + \\exp(y)) = a + \\log(\\exp(x - a) + \\exp(y - a))$, where $a = \\max(x, y)$\n",
        "- Now, our final functional version becomes:  \n",
        "  $\\log\\left(\\frac{\\exp(x)}{\\exp(x) + \\exp(y)}\\right) = x - \\left[ a + \\log(\\exp(x - a) + \\exp(y - a)) \\right]$\n",
        "\n",
        "<font color='blue'>**Key point**:</font> This approach helps us avoid numerical overflow by normalizing large exponentials.\n",
        "\n",
        "\n",
        "                                                       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alymet8aC3c4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.305595Z",
          "start_time": "2019-01-29T22:48:56.293399Z"
        },
        "id": "alymet8aC3c4"
      },
      "outputs": [],
      "source": [
        "def stable_log_exp(x, y):\n",
        "    # define a\n",
        "    a = torch.max(x,y)\n",
        "\n",
        "    # define e^x - a and e^y-a\n",
        "    exp_x = torch.exp(x-a)\n",
        "\n",
        "    exp_y = torch.exp(y-a)\n",
        "\n",
        "    # calculating a + our logsum trick\n",
        "    denom = a + torch.log(exp_x + exp_y)\n",
        "\n",
        "    # final calculation (multiply by -1 for loss)\n",
        "    return (-1)*(x-denom)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb3df66-1b98-4aa9-ae8a-aef12c12fbd8",
      "metadata": {
        "id": "adb3df66-1b98-4aa9-ae8a-aef12c12fbd8"
      },
      "source": [
        "Here we have an overflow calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZsrgvsFh8UFq",
      "metadata": {
        "id": "ZsrgvsFh8UFq",
        "scrolled": true,
        "outputId": "e0b7c627-8296-4977-9352-e217e268de5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([inf], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_exp(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee9857bb-d037-44d5-8042-3b2a56355faf",
      "metadata": {
        "id": "ee9857bb-d037-44d5-8042-3b2a56355faf"
      },
      "source": [
        "Here is the stable calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3NoH9aa48Lzf",
      "metadata": {
        "id": "3NoH9aa48Lzf",
        "outputId": "16c2bb5f-0290-4cec-b72f-ab52f29fc0ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([50.], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stable_log_exp(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0308103b-eab6-454a-b7b6-2f1c23f556d9",
      "metadata": {
        "id": "0308103b-eab6-454a-b7b6-2f1c23f556d9"
      },
      "source": [
        "Calling the gradient function to calculate the gradients using the stable calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1uQXi8AC3c5",
      "metadata": {
        "id": "d1uQXi8AC3c5",
        "outputId": "5479e781-f9c8-4180-eaf1-142296a4afbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad = tensor([-1.])\n",
            "y.grad = tensor([1.])\n"
          ]
        }
      ],
      "source": [
        "grad(stable_log_exp, x, y)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (condaenv)",
      "language": "python",
      "name": "condaenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}