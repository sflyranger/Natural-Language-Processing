# Natural Language Processing (NLP) Specialization

Natural Language Processing (NLP) is a field focused on enabling machines to understand and manipulate human language. As a key area within machine learning, NLP is widely applied in various domains, from analyzing text to building conversational agents. This Specialization offers a comprehensive approach to mastering NLP, covering both fundamental techniques and advanced deep learning models.

## Course Breakdown

### Course 1: Classification and Vector Spaces in NLP
This course introduces essential concepts in NLP with a focus on classification methods and vector space models.

- **Week 1:** *Logistic Regression for Sentiment Analysis of Tweets*  
  Learn to classify tweets based on sentiment using logistic regression.
- **Week 2:** *Naïve Bayes for Sentiment Analysis of Tweets*  
  Explore sentiment analysis using the Naïve Bayes classifier.
- **Week 3:** *Vector Space Models*  
  Study relationships between words using vector spaces and apply Principal Component Analysis (PCA) for dimensionality reduction.
- **Week 4:** *Word Embeddings and Locality Sensitive Hashing for Machine Translation*  
  Implement a simple English-to-French translation algorithm using word embeddings and locality-sensitive hashing.

### Course 2: Probabilistic Models in NLP
This course delves into probabilistic models, emphasizing their application in various NLP tasks.

- **Week 1:** *Auto-correct using Minimum Edit Distance*  
  Develop an auto-correct algorithm based on minimum edit distance and dynamic programming.
- **Week 2:** *Part-of-Speech (POS) Tagging*  
  Apply the Viterbi algorithm for tagging parts of speech in text.
- **Week 3:** *N-gram Language Models*  
  Build an N-gram model to improve auto-complete functionality, with applications in translation and speech recognition.
- **Week 4:** *Word2Vec and Stochastic Gradient Descent*  
  Construct a Word2Vec model using neural networks and stochastic gradient descent.

### Course 3: Sequence Models in NLP
This course covers sequence models, focusing on how words and their sequences are processed in NLP.

- **Week 1:** *Sentiment with Neural Nets*  
  Train a neural network using GLoVe word embeddings for sentiment analysis.
- **Week 2:** *Language Generation Models*  
  Generate text using a Gated Recurrent Unit (GRU) language model.
- **Week 3:** *Named Entity Recognition (NER)*  
  Implement Named Entity Recognition using a recurrent neural network with LSTM layers.
- **Week 4:** *Siamese Networks*  
  Use Siamese LSTM models to compare and analyze questions for similarity.

### Course 4: Attention Models in NLP
The final course explores attention mechanisms and transformer models, which are central to modern NLP tasks.

- **Week 1:** *Neural Machine Translation with Attention*  
  Translate sentences using an encoder-decoder model with attention.
- **Week 2:** *Summarization with Transformer Models*  
  Build a transformer model to create text summaries.
- **Week 3:** *Question-Answering with Transformer Models*  
  Implement question-answering systems using T5 and BERT models.
- **Week 4:** *Chatbots with a Reformer Model*  
  Design and implement a chatbot using a reformer model.

